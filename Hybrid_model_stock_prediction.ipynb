{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpFLwXeSlCdB/5Z6oiNj8d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhtejn/Stock_AI_ML/blob/main/Hybrid_model_stock_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbtjFR9rUutg"
      },
      "outputs": [],
      "source": [
        "# Download TA-Lib\n",
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "\n",
        "!ls\n",
        "!tar xvzf ta-lib-0.4.0-src.tar.gz\n",
        "!ls\n",
        "\n",
        "import os\n",
        "os.chdir('ta-lib') # Can't use !cd in co-lab\n",
        "\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "\n",
        "# wait ~ 30s\n",
        "os.chdir('../')\n",
        "!ls\n",
        "\n",
        "!pip install TA-Lib\n",
        "import talib\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install imbalanced-learn\n",
        "import yfinance as yf\n",
        "import talib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Reshape\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# List of stock tickers to fetch data for\n",
        "tickers = ['TATAMOTORS.NS', 'M&M.NS', 'MARUTI.NS']  # Add more tickers as needed\n",
        "\n",
        "# Function to fetch historical stock data\n",
        "def fetch_stock_data(tickers, start, end):\n",
        "    data = {}\n",
        "    for ticker in tickers:\n",
        "        df = yf.download(ticker, start=start, end=end)\n",
        "        df['Ticker'] = ticker\n",
        "        data[ticker] = df\n",
        "    return pd.concat(data.values())\n",
        "\n",
        "# Fetch data\n",
        "start_date='2019-08-01'\n",
        "end_date='2024-07-31'\n",
        "data = fetch_stock_data(tickers, start_date, end_date)\n",
        "\n",
        "# Calculate technical indicators using TA-Lib\n",
        "def calculate_indicators(df):\n",
        "    df['EMA20'] = talib.EMA(df['Close'], timeperiod=20)\n",
        "    df['EMA50'] = talib.EMA(df['Close'], timeperiod=50)\n",
        "    df['EMA100'] = talib.EMA(df['Close'], timeperiod=100)\n",
        "    df['EMA200'] = talib.EMA(df['Close'], timeperiod=200)\n",
        "    df['RSI'] = talib.RSI(df['Close'])\n",
        "    df['MACD'], df['MACDSignal'], _ = talib.MACD(df['Close'])\n",
        "    df['STOCHF'], _ = talib.STOCHF(df['High'], df['Low'], df['Close'])\n",
        "    df['CCI'] = talib.CCI(df['High'], df['Low'], df['Close'])\n",
        "    df['ADX'] = talib.ADX(df['High'], df['Low'], df['Close'])\n",
        "    df['CMO'] = talib.CMO(df['Close'])\n",
        "    df['MOM'] = talib.MOM(df['Close'])\n",
        "    df['WILLR'] = talib.WILLR(df['High'], df['Low'], df['Close'])\n",
        "    df['MFI'] = talib.MFI(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "    df['ATR'] = talib.ATR(df['High'], df['Low'], df['Close'])\n",
        "    df['BOLL'], _, _ = talib.BBANDS(df['Close'])\n",
        "    df['AD'] = talib.AD(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "    df['OBV'] = talib.OBV(df['Close'], df['Volume'])\n",
        "    # VWAP is not directly available in TA-Lib, so calculate manually\n",
        "    df['VWAP'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum()\n",
        "    return df\n",
        "\n",
        "# Function to create labels for a single ticker's DataFrame\n",
        "def create_labels(df):\n",
        "    df['Label'] = 'Hold'\n",
        "    # Set the window for future price comparison\n",
        "    days = 5\n",
        "\n",
        "    for i in range(len(df) - days):\n",
        "        current_price = df.iloc[i]['Close']\n",
        "        future_price = df.iloc[i + days]['Close']\n",
        "        price_change = (future_price - current_price) / current_price\n",
        "\n",
        "        # Check if conditions for Buy or Sell are met\n",
        "        if price_change >= 0.05:\n",
        "            df.at[df.index[i], 'Label'] = 'Buy'\n",
        "        elif price_change <= -0.05:\n",
        "            df.at[df.index[i], 'Label'] = 'Sell'\n",
        "\n",
        "    return df\n",
        "\n",
        "# Process each ticker separately\n",
        "def process_tickers(tickers):\n",
        "    all_data = []\n",
        "    for ticker in tickers:\n",
        "        df = data[data['Ticker'] == ticker].copy()\n",
        "        df = calculate_indicators(df)\n",
        "        df = create_labels(df)\n",
        "        df['Ticker'] = ticker  # Keep ticker information\n",
        "        all_data.append(df)\n",
        "    return pd.concat(all_data)\n",
        "\n",
        "# Process data\n",
        "data = process_tickers(tickers)\n",
        "\n",
        "# Prepare features and labels\n",
        "features = ['Open', 'High', 'Low', 'Close', 'Volume', 'EMA20', 'EMA50', 'EMA100', 'EMA200', 'RSI', 'MACD', 'MACDSignal', 'STOCHF', 'CCI', 'ADX', 'CMO', 'MOM', 'WILLR', 'MFI', 'ATR', 'BOLL', 'AD', 'OBV', 'VWAP']\n",
        "X = data[features].fillna(0)\n",
        "y = data['Label']\n",
        "\n",
        "# Encode labels\n",
        "y = np.array(y)  # Ensure y is a numpy array\n",
        "\n",
        "# Label encoding for categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Balance classes if necessary\n",
        "# Perform undersampling\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_resampled_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train_resampled_cat = to_categorical(y_train_resampled)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_resampled_scaled.shape[1], 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.5),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(3, activation='softmax')  # Assuming 3 classes: Hold, Buy, Sell\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Reshape data for CNN input\n",
        "X_train_resampled_reshaped = np.expand_dims(X_train_resampled_scaled, axis=2)\n",
        "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_resampled_reshaped, y_train_resampled_cat, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test_cat, axis=1)\n",
        "\n",
        "print('Neural Network Accuracy:', accuracy_score(y_test_classes, y_pred_classes))\n",
        "print('Neural Network Classification Report:\\n', classification_report(y_test_classes, y_pred_classes, target_names=['Hold', 'Buy', 'Sell']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eEmlaOvc5y9",
        "outputId": "064db27b-a5eb-4b73-aa48-b3236dac364e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 60ms/step - accuracy: 0.3246 - loss: 1.0998 - val_accuracy: 0.0000e+00 - val_loss: 1.4066\n",
            "Epoch 2/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.3822 - loss: 1.0537 - val_accuracy: 0.0000e+00 - val_loss: 1.6961\n",
            "Epoch 3/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4111 - loss: 1.0345 - val_accuracy: 0.0000e+00 - val_loss: 1.7110\n",
            "Epoch 4/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4145 - loss: 1.0548 - val_accuracy: 0.0000e+00 - val_loss: 1.6430\n",
            "Epoch 5/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.3972 - loss: 1.0314 - val_accuracy: 0.0000e+00 - val_loss: 1.6802\n",
            "Epoch 6/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4200 - loss: 1.0326 - val_accuracy: 0.0000e+00 - val_loss: 1.6884\n",
            "Epoch 7/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4114 - loss: 1.0326 - val_accuracy: 0.0000e+00 - val_loss: 1.7776\n",
            "Epoch 8/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4377 - loss: 1.0492 - val_accuracy: 0.0000e+00 - val_loss: 1.7336\n",
            "Epoch 9/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4564 - loss: 1.0146 - val_accuracy: 0.0000e+00 - val_loss: 1.6492\n",
            "Epoch 10/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4378 - loss: 1.0310 - val_accuracy: 0.0000e+00 - val_loss: 1.7042\n",
            "Epoch 11/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3921 - loss: 1.0331 - val_accuracy: 0.0000e+00 - val_loss: 1.8756\n",
            "Epoch 12/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4572 - loss: 1.0148 - val_accuracy: 0.0000e+00 - val_loss: 1.4778\n",
            "Epoch 13/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4270 - loss: 1.0402 - val_accuracy: 0.0000e+00 - val_loss: 1.7952\n",
            "Epoch 14/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.3946 - loss: 1.0425 - val_accuracy: 0.0000e+00 - val_loss: 1.6656\n",
            "Epoch 15/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4756 - loss: 1.0187 - val_accuracy: 0.0000e+00 - val_loss: 1.6569\n",
            "Epoch 16/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4950 - loss: 1.0204 - val_accuracy: 0.0000e+00 - val_loss: 1.6180\n",
            "Epoch 17/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4785 - loss: 1.0242 - val_accuracy: 0.0000e+00 - val_loss: 1.7201\n",
            "Epoch 18/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4459 - loss: 1.0254 - val_accuracy: 0.0000e+00 - val_loss: 1.5247\n",
            "Epoch 19/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4524 - loss: 1.0236 - val_accuracy: 0.0000e+00 - val_loss: 1.7321\n",
            "Epoch 20/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4734 - loss: 1.0310 - val_accuracy: 0.0000e+00 - val_loss: 1.5980\n",
            "Epoch 21/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4664 - loss: 1.0175 - val_accuracy: 0.0000e+00 - val_loss: 1.6690\n",
            "Epoch 22/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4634 - loss: 1.0147 - val_accuracy: 0.0000e+00 - val_loss: 1.8182\n",
            "Epoch 23/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4751 - loss: 1.0184 - val_accuracy: 0.0000e+00 - val_loss: 1.7568\n",
            "Epoch 24/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.4497 - loss: 1.0395 - val_accuracy: 0.0000e+00 - val_loss: 1.6928\n",
            "Epoch 25/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4651 - loss: 1.0589 - val_accuracy: 0.0000e+00 - val_loss: 1.7483\n",
            "Epoch 26/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4853 - loss: 1.0095 - val_accuracy: 0.0000e+00 - val_loss: 1.6628\n",
            "Epoch 27/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4437 - loss: 1.0396 - val_accuracy: 0.0000e+00 - val_loss: 1.7136\n",
            "Epoch 28/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4492 - loss: 1.0093 - val_accuracy: 0.0000e+00 - val_loss: 1.7467\n",
            "Epoch 29/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4977 - loss: 0.9923 - val_accuracy: 0.0000e+00 - val_loss: 1.6515\n",
            "Epoch 30/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4395 - loss: 1.0178 - val_accuracy: 0.0000e+00 - val_loss: 1.6545\n",
            "Epoch 31/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4867 - loss: 0.9991 - val_accuracy: 0.0000e+00 - val_loss: 1.7823\n",
            "Epoch 32/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4829 - loss: 1.0113 - val_accuracy: 0.0000e+00 - val_loss: 1.6726\n",
            "Epoch 33/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4753 - loss: 1.0193 - val_accuracy: 0.0000e+00 - val_loss: 1.7104\n",
            "Epoch 34/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4750 - loss: 1.0089 - val_accuracy: 0.0000e+00 - val_loss: 1.7222\n",
            "Epoch 35/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.4597 - loss: 1.0163 - val_accuracy: 0.0000e+00 - val_loss: 1.7065\n",
            "Epoch 36/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.4633 - loss: 1.0350 - val_accuracy: 0.0000e+00 - val_loss: 1.6122\n",
            "Epoch 37/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4316 - loss: 1.0473 - val_accuracy: 0.0000e+00 - val_loss: 1.6901\n",
            "Epoch 38/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4616 - loss: 1.0014 - val_accuracy: 0.0000e+00 - val_loss: 1.7636\n",
            "Epoch 39/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4933 - loss: 0.9968 - val_accuracy: 0.0000e+00 - val_loss: 1.7566\n",
            "Epoch 40/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4887 - loss: 1.0298 - val_accuracy: 0.0000e+00 - val_loss: 1.7306\n",
            "Epoch 41/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.5075 - loss: 0.9957 - val_accuracy: 0.0000e+00 - val_loss: 1.6403\n",
            "Epoch 42/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4514 - loss: 1.0155 - val_accuracy: 0.0000e+00 - val_loss: 1.6123\n",
            "Epoch 43/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4874 - loss: 0.9995 - val_accuracy: 0.0000e+00 - val_loss: 1.9521\n",
            "Epoch 44/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5032 - loss: 1.0116 - val_accuracy: 0.0000e+00 - val_loss: 1.5396\n",
            "Epoch 45/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.4691 - loss: 1.0123 - val_accuracy: 0.0000e+00 - val_loss: 1.7610\n",
            "Epoch 46/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4795 - loss: 0.9965 - val_accuracy: 0.0000e+00 - val_loss: 1.7322\n",
            "Epoch 47/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4826 - loss: 0.9871 - val_accuracy: 0.0000e+00 - val_loss: 1.7273\n",
            "Epoch 48/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4622 - loss: 1.0173 - val_accuracy: 0.0000e+00 - val_loss: 1.8881\n",
            "Epoch 49/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4915 - loss: 0.9954 - val_accuracy: 0.0000e+00 - val_loss: 1.6869\n",
            "Epoch 50/50\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4851 - loss: 0.9940 - val_accuracy: 0.0000e+00 - val_loss: 1.5138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 33 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7e7cd97609d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "Neural Network Accuracy: 0.43243243243243246\n",
            "Neural Network Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        Hold       0.20      0.82      0.32       114\n",
            "         Buy       0.86      0.40      0.54       571\n",
            "        Sell       0.00      0.00      0.00        55\n",
            "\n",
            "    accuracy                           0.43       740\n",
            "   macro avg       0.35      0.41      0.29       740\n",
            "weighted avg       0.70      0.43      0.47       740\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, LSTM, GRU, Input, Attention\n",
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fetch historical OHLCV data for given stock tickers\n",
        "def fetch_data(tickers, start, end):\n",
        "    dfs = []\n",
        "    for ticker in tickers:\n",
        "        df = yf.download(ticker, start=start, end=end)  # Download data from Yahoo Finance\n",
        "        df['Ticker'] = ticker  # Add ticker as a column\n",
        "        df.reset_index(inplace=True)  # Ensure 'Date' is a column\n",
        "        dfs.append(df)  # Append dataframe to the list\n",
        "    return pd.concat(dfs, ignore_index=True)  # Concatenate all dataframes and reset index\n",
        "\n",
        "# Calculate Exponential Moving Average (EMA)\n",
        "def calculate_ema(df, column, period):\n",
        "    return df[column].ewm(span=period, adjust=False).mean()\n",
        "\n",
        "# Calculate MACD and MACD Signal\n",
        "def calculate_macd(df):\n",
        "    df['EMA12'] = calculate_ema(df, 'Close', 12)\n",
        "    df['EMA26'] = calculate_ema(df, 'Close', 26)\n",
        "    df['MACD'] = df['EMA12'] - df['EMA26']\n",
        "    df['MACD Signal'] = calculate_ema(df, 'MACD', 9)\n",
        "    return df\n",
        "\n",
        "# Calculate Bollinger Bands\n",
        "def calculate_bollinger_bands(df, column, window=20, num_sd=2):\n",
        "    rolling_mean = df[column].rolling(window=window).mean()\n",
        "    rolling_std = df[column].rolling(window=window).std()\n",
        "    df['Bollinger High'] = rolling_mean + (rolling_std * num_sd)\n",
        "    df['Bollinger Low'] = rolling_mean - (rolling_std * num_sd)\n",
        "    return df\n",
        "\n",
        "# Calculate VWAP (Volume Weighted Average Price)\n",
        "def calculate_vwap(df):\n",
        "    df['Cumulative_Price_Volume'] = (df['Close'] * df['Volume']).cumsum()\n",
        "    df['Cumulative_Volume'] = df['Volume'].cumsum()\n",
        "    df['VWAP'] = df['Cumulative_Price_Volume'] / df['Cumulative_Volume']\n",
        "    return df\n",
        "\n",
        "# Calculate Relative Strength Index (RSI)\n",
        "def calculate_rsi(df, column, period=14):\n",
        "    delta = df[column].diff(1)\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
        "    rs = gain / loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "    return df\n",
        "\n",
        "# Apply technical indicators to DataFrame ticker-wise\n",
        "def apply_indicators(df):\n",
        "    grouped = df.groupby('Ticker')  # Group data by ticker\n",
        "\n",
        "    def apply_group(group):\n",
        "        group = group.sort_values(by='Date').drop_duplicates(subset=['Date'])  # Ensure no duplicate dates\n",
        "        group['EMA20'] = calculate_ema(group, 'Close', 20)\n",
        "        group['EMA50'] = calculate_ema(group, 'Close', 50)\n",
        "        group['EMA100'] = calculate_ema(group, 'Close', 100)\n",
        "        group = calculate_macd(group)\n",
        "        group = calculate_bollinger_bands(group, 'Close')\n",
        "        group = calculate_vwap(group)\n",
        "        group = calculate_rsi(group, 'Close')\n",
        "        return group\n",
        "\n",
        "    result_df = grouped.apply(apply_group).reset_index(drop=True)  # Apply to each group and reset index\n",
        "    return result_df\n",
        "\n",
        "# Create labels for the classification task\n",
        "\n",
        "'''\n",
        "def create_labels(df):\n",
        "    df['Future Price'] = df.groupby('Ticker')['Close'].shift(-5)\n",
        "    df['Price Change'] = (df['Future Price'] - df['Close']) / df['Close']\n",
        "    df['Buy'] = df['Price Change'] >= 0.02\n",
        "    df['Sell'] = df['Price Change'] <= -0.02\n",
        "    df['Label'] = np.where(df['Buy'], 1, np.where(df['Sell'], 2, 0))  # Label encoding: Buy=1, Sell=2, Hold=0\n",
        "    df.drop(columns=['Future Price', 'Price Change'], inplace=True)\n",
        "    return df\n",
        "'''\n",
        "# labels using rsi\n",
        "def create_labels(df):\n",
        "    df['Buy'] = (df['RSI'] > 65) & (df['RSI'].diff() > 0)\n",
        "    df['Sell'] = (df['RSI'] < 35) & (df['RSI'].diff() < 0)\n",
        "    df['Label'] = np.where(df['Buy'], 1, np.where(df['Sell'], 2, 0))  # Label encoding: Buy=1, Sell=2, Hold=0\n",
        "    return df\n",
        "\n",
        "\n",
        "# Preprocess data: label creation and scaling\n",
        "def preprocess_data(df):\n",
        "    # Create labels\n",
        "    df = create_labels(df)\n",
        "\n",
        "    # Drop non-numeric columns for feature scaling\n",
        "    df.drop(columns=['Date', 'Ticker'], inplace=True)\n",
        "\n",
        "    # Drop any rows with NaN values\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = MinMaxScaler()\n",
        "    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'EMA20', 'EMA50', 'EMA100', 'MACD', 'MACD Signal', 'Bollinger High', 'Bollinger Low', 'VWAP', 'RSI']\n",
        "    df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "    return df, scaler\n",
        "\n",
        "# Custom Attention Layer\n",
        "class CustomAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, value = inputs\n",
        "        attention = tf.keras.layers.Attention()([query, value])\n",
        "        return tf.reduce_sum(attention, axis=1)\n",
        "\n",
        "def build_hybrid_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # CNN Layers\n",
        "    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    # RNN Layers\n",
        "    x = LSTM(100, return_sequences=True)(x)\n",
        "    x = LSTM(50, return_sequences=True)(x)\n",
        "\n",
        "    # Attention Mechanism\n",
        "    query = Dense(50)(x)\n",
        "    value = Dense(50)(x)\n",
        "    attention_output = CustomAttention()([query, value])\n",
        "\n",
        "    # Dense Layers\n",
        "    x = Dense(50, activation='relu')(attention_output)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(3, activation='softmax')(x)  # Output layer for classification\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    # Define tickers\n",
        "    tickers = ['DRREDDY.NS', 'HINDALCO.NS', 'JSWSTEEL.NS', 'TATAMOTORS.NS', 'M&M.NS', 'ONGC.NS', 'COALINDIA.NS', 'TECHM.NS', 'UPL.NS', 'TATASTEEL.NS',\n",
        "               'TATACONSUM.NS', 'ITC.NS', 'ADANIENT.NS', 'TITAN.NS', 'EICHERMOT.NS', 'ADANIPORTS.NS', 'RELIANCE.NS', 'BPCL.NS', 'LT.NS', 'NTPC.NS',\n",
        "               'LTIM.NS', 'BAJAJ-AUTO.NS', 'SUNPHARMA.NS', 'INFY.NS', 'WIPRO.NS', 'INDUSINDBK.NS', 'HCLTECH.NS', 'SBIN.NS', 'KOTAKBANK.NS', 'HDFCLIFE.NS',\n",
        "               'BAJAJFINSV.NS', 'SBILIFE.NS', 'BRITANNIA.NS', 'HDFCBANK.NS', 'CIPLA.NS', 'GRASIM.NS', 'NESTLEIND.NS', 'BHARTIARTL.NS', 'TCS.NS',\n",
        "               'AXISBANK.NS', 'HEROMOTOCO.NS', 'HINDUNILVR.NS', 'ASIANPAINT.NS', 'POWERGRID.NS', 'ULTRACEMCO.NS', 'ICICIBANK.NS', 'APOLLOHOSP.NS',\n",
        "               'MARUTI.NS', 'BAJFINANCE.NS', 'DIVISLAB.NS']\n",
        "\n",
        "    # Download historical stock data\n",
        "    start_date = '2019-08-01'\n",
        "    end_date = '2024-07-31'\n",
        "    df = fetch_data(tickers, start=start_date, end=end_date)\n",
        "\n",
        "    # Apply technical indicators\n",
        "    df = apply_indicators(df)\n",
        "\n",
        "    # Preprocess data\n",
        "    df, scaler = preprocess_data(df)\n",
        "\n",
        "    # Define features and labels\n",
        "    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'EMA20', 'EMA50', 'EMA100', 'MACD', 'MACD Signal', 'Bollinger High', 'Bollinger Low', 'VWAP', 'RSI']\n",
        "    X = df[features].values\n",
        "    y = df['Label'].values\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Reshape for CNN (samples, time steps, features)\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    # Build and train the model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    model = build_hybrid_model(input_shape)\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Save the model and scaler\n",
        "    model.save('hybrid_model.h5')\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqsgO23atprX",
        "outputId": "7de6cc4a-4dd4-4b84-a347-b260a5e7d2b8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 18ms/step - accuracy: 0.7505 - loss: 0.7186 - val_accuracy: 0.8418 - val_loss: 0.3174\n",
            "Epoch 2/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 17ms/step - accuracy: 0.8106 - loss: 0.3341 - val_accuracy: 0.8390 - val_loss: 0.2793\n",
            "Epoch 3/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 18ms/step - accuracy: 0.8157 - loss: 0.3297 - val_accuracy: 0.8336 - val_loss: 0.3166\n",
            "Epoch 4/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 19ms/step - accuracy: 0.8214 - loss: 0.3112 - val_accuracy: 0.8453 - val_loss: 0.2787\n",
            "Epoch 5/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 18ms/step - accuracy: 0.8210 - loss: 0.3113 - val_accuracy: 0.8398 - val_loss: 0.2799\n",
            "Epoch 6/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 19ms/step - accuracy: 0.8230 - loss: 0.3111 - val_accuracy: 0.8180 - val_loss: 0.3099\n",
            "Epoch 7/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 20ms/step - accuracy: 0.8236 - loss: 0.3077 - val_accuracy: 0.7939 - val_loss: 0.3392\n",
            "Epoch 8/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 18ms/step - accuracy: 0.8223 - loss: 0.3087 - val_accuracy: 0.8433 - val_loss: 0.2950\n",
            "Epoch 9/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 18ms/step - accuracy: 0.8253 - loss: 0.3041 - val_accuracy: 0.8499 - val_loss: 0.2872\n",
            "Epoch 10/10\n",
            "\u001b[1m1366/1366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 19ms/step - accuracy: 0.8257 - loss: 0.3054 - val_accuracy: 0.8313 - val_loss: 0.2792\n",
            "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8110378912685338\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87      9036\n",
            "           1       0.59      0.90      0.71      2007\n",
            "           2       0.67      0.44      0.53      1097\n",
            "\n",
            "    accuracy                           0.81     12140\n",
            "   macro avg       0.72      0.72      0.70     12140\n",
            "weighted avg       0.83      0.81      0.81     12140\n",
            "\n"
          ]
        }
      ]
    }
  ]
}